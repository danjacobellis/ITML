{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b3d938-1517-4efe-93c9-78c20d6f354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 08:30:56.422129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 08:30:56.567948: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 08:30:57.173545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 08:30:57.173605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 08:30:57.173612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import larq as lq\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9e833c-8ea0-4652-ae0f-7ea711b8460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ed2bd5-24ca-4894-a9fd-1524ab600c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=4):\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(latent_dim=4):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(latent_dim).output.shape[1:])\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        latent_inputs\n",
    "    )\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fcd33b-c392-4821-b9a2-d16c2f0cc9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 08:30:58.610925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:58.647537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:58.647773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:58.648270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 08:30:58.649203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:58.649614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:58.649983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:59.146939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:59.147174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:59.147353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 08:30:59.147503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10648 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vq_vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 7, 7, 4)           19076     \n",
      "                                                                 \n",
      " vector_quantizer (VectorQua  (None, 7, 7, 4)          64        \n",
      " ntizer)                                                         \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         21121     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,261\n",
      "Trainable params: 40,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_vqvae(latent_dim=4, num_embeddings=16):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cecf4c-32aa-4927-a450-1df89c9c1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=4, num_embeddings=16, **kwargs):\n",
    "        super(VQVAETrainer, self).__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb408367-0bbb-490d-bc6a-13f3c8906502",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_train_scaled = (x_train / 255.0) - 0.5\n",
    "x_test_scaled = (x_test / 255.0) - 0.5\n",
    "data_variance = np.var(x_train / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df1d126e-574c-49ac-b596-734a07569c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.3554 - reconstruction_loss: 0.3638 - vqvae_loss: 1.6514\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.5268 - reconstruction_loss: 0.1546 - vqvae_loss: 1.3625\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.5974 - reconstruction_loss: 0.1241 - vqvae_loss: 0.4697\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.5112 - reconstruction_loss: 0.1154 - vqvae_loss: 0.3944\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.4903 - reconstruction_loss: 0.1102 - vqvae_loss: 0.3790\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.4740 - reconstruction_loss: 0.1068 - vqvae_loss: 0.3664\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.4521 - reconstruction_loss: 0.1027 - vqvae_loss: 0.3478\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3975 - reconstruction_loss: 0.0962 - vqvae_loss: 0.3002\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3776 - reconstruction_loss: 0.0931 - vqvae_loss: 0.2840\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3692 - reconstruction_loss: 0.0912 - vqvae_loss: 0.2776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7cf86da050>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=4, num_embeddings=16)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "vqvae_trainer.fit(x_train_scaled, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54b277ce-b5ee-45a1-a8b1-1f1676f2349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "encoder = vqvae_trainer.vqvae.get_layer(\"encoder\")\n",
    "decoder = vqvae_trainer.vqvae.get_layer(\"decoder\")\n",
    "quantizer = vqvae_trainer.vqvae.get_layer(\"vector_quantizer\")\n",
    "\n",
    "# encoded_outputs = encoder.predict(test_images)\n",
    "# flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "# codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "# codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "\n",
    "encoded_outputs = encoder.predict(x_train)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "e_train = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "# e_train = keras.utils.to_categorical(e_train)\n",
    "\n",
    "encoded_outputs = encoder.predict(x_test)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "e_test = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "# e_test = keras.utils.to_categorical(e_test)\n",
    "\n",
    "# quantizer.\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# x_train = np.expand_dims(x_train, -1)\n",
    "# x_test = np.expand_dims(x_test, -1)\n",
    "# x_train_scaled = (x_train / 255.0) - 0.5\n",
    "# x_test_scaled = (x_test / 255.0) - 0.5\n",
    "# data_variance = np.var(x_train / 255.0)\n",
    "\n",
    "# (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "# test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "# train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca35635c-f3ef-438d-a90e-02a6bdd7ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = np.shape(e_train)[0];\n",
    "# b_train = np.zeros((N,28,28))\n",
    "# for i_sample in range(N):\n",
    "#     for x in range(7):\n",
    "#         for y in range(7):\n",
    "#             b_train[i_sample][4*x:4*x+4,4*y:4*y+4] = 2*np.reshape(e_train[i_sample][x,y],(4,4))-1\n",
    "# N = np.shape(e_test)[0];\n",
    "# b_test = np.zeros((N,28,28))\n",
    "# for i_sample in range(N):\n",
    "#     for x in range(7):\n",
    "#         for y in range(7):\n",
    "#             b_test[i_sample][4*x:4*x+4,4*y:4*y+4] = 2*np.reshape(e_test[i_sample][x,y],(4,4))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb36cc11-778d-44c5-af11-bd9bbafc7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.shape(e_train)[0];\n",
    "b_train = np.zeros((N,7,7,4))\n",
    "for i_sample in range(N):\n",
    "    for x_block in range(7):\n",
    "        for y_block in range(7):\n",
    "            word = e_train[i_sample][x_block,y_block]\n",
    "            for i_bit in range(4):\n",
    "                b_train[i_sample][x_block,y_block][i_bit] = (word >> i_bit) & 1\n",
    "N = np.shape(e_test)[0];\n",
    "b_test = np.zeros((N,7,7,4))\n",
    "for i_sample in range(N):\n",
    "    for x_block in range(7):\n",
    "        for y_block in range(7):\n",
    "            word = e_test[i_sample][x_block,y_block]\n",
    "            for i_bit in range(4):\n",
    "                b_test[i_sample][x_block,y_block][i_bit] = (word >> i_bit) & 1\n",
    "b_train, b_test = 2*b_train - 1, 2*b_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "29ba58c9-a97b-4b23-87f1-3c30d80c7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quantized layers except the first will use the same options\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# In the first layer we only quantize the weights and not the input\n",
    "model.add(lq.layers.QuantConv2D(64, (7, 7),\n",
    "                                # kernel_quantizer=\"ste_sign\",\n",
    "                                # kernel_constraint=\"weight_clip\",\n",
    "                                input_quantizer=\"ste_sign\",\n",
    "                                use_bias=False,\n",
    "                                input_shape=(7, 7, 4)))\n",
    "# model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "# model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "# model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "# model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "855c9384-d1bf-4852-af57-b9be1b3b82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+sequential_30 stats--------------------------------------------------------------------------------------+\n",
      "| Layer                   Input prec.         Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs  32-bit MACs |\n",
      "|                               (bit)                      x 1       x 1    (kB)                          |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "| quant_conv2d_54                   1  (-1, 1, 1, 64)        0     12544   49.00           0        12544 |\n",
      "| batch_normalization_94            -  (-1, 1, 1, 64)        0       128    0.50           0            0 |\n",
      "| flatten_21                        -        (-1, 64)        0         0       0           0            0 |\n",
      "| quant_dense_44                    1        (-1, 64)     4096         0    0.50        4096            0 |\n",
      "| batch_normalization_95            -        (-1, 64)        0       128    0.50           0            0 |\n",
      "| quant_dense_45                    1        (-1, 10)      640         0    0.08         640            0 |\n",
      "| batch_normalization_96            -        (-1, 10)        0        20    0.08           0            0 |\n",
      "| activation_22                     -        (-1, 10)        0         0       0           ?            ? |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                   4736     12820   50.66        4736        12544 |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "+sequential_30 summary------------------------+\n",
      "| Total params                      17.6 k    |\n",
      "| Trainable params                  17.3 k    |\n",
      "| Non-trainable params              276       |\n",
      "| Model size                        50.66 KiB |\n",
      "| Model size (8-bit FP weights)     13.10 KiB |\n",
      "| Float-32 Equivalent               68.58 KiB |\n",
      "| Compression Ratio of Memory       0.74      |\n",
      "| Number of MACs                    17.3 k    |\n",
      "| Ratio of MACs that are binarized  0.2741    |\n",
      "+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "13e9ca60-86fc-44eb-86f0-29183fbf5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.9628 - accuracy: 0.7906\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.8328 - accuracy: 0.8363\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.8145 - accuracy: 0.8420\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.8017 - accuracy: 0.8472\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7944 - accuracy: 0.8487\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7904 - accuracy: 0.8500\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7903 - accuracy: 0.8498\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7826 - accuracy: 0.8524\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7837 - accuracy: 0.8521\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7835 - accuracy: 0.8523\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7775 - accuracy: 0.8545\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7795 - accuracy: 0.8540\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7781 - accuracy: 0.8540\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7739 - accuracy: 0.8561\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7744 - accuracy: 0.8548\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7764 - accuracy: 0.8554\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7733 - accuracy: 0.8565\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7716 - accuracy: 0.8564\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7725 - accuracy: 0.8564\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7709 - accuracy: 0.8567\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7114 - accuracy: 0.8757\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(b_train, y_train, batch_size=64, epochs=20)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(b_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "563c0788-77c6-4e51-93fa-f91362446e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 87.57 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy {test_acc * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

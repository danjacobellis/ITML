
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lossy Compression and Learning &#8212; Dan Jacobellis | University of Texas at Austin</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Compressed Learning" href="milestone.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="lossy-compression-and-learning">
<h1>Lossy Compression and Learning<a class="headerlink" href="#lossy-compression-and-learning" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://danjacobellis.github.io/ITML/lossy_learning.slides.html">Slides</a></p>
<section id="lossy-compression">
<h2>Lossy compression<a class="headerlink" href="#lossy-compression" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Most data are stored using lossy formats (MP3, JPEG)</p></li>
<li><p>1-4 bit subband quantization is typical</p></li>
<li><p>~1.5 bits per sample/pixel after entropy coding</p></li>
</ul>
<p><img alt="" src="_images/lossy_lossless.png" /></p>
</section>
<section id="conventional-training-procedure">
<h2>Conventional Training Procedure<a class="headerlink" href="#conventional-training-procedure" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Still suffers from all of the downsides of lossy compression</p></li>
<li><p>Don’t get any of the benefits of smaller representation!</p></li>
</ul>
<p><img alt="" src="_images/conventional.png" /></p>
</section>
<section id="training-on-transformed-data">
<h2>Training on transformed data<a class="headerlink" href="#training-on-transformed-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Reduce size and number of initial convolutional layers by training on DCT coefficients</p></li>
<li><p>Input size remains the same (Quantization of transform coefficients ignored)</p></li>
</ul>
<p><img alt="" src="_images/dct_vs_resnet.png" /></p>
</section>
<section id="id1">
<h2>Training on transformed data<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="_images/FNNSFJ.png" /></p>
</section>
<section id="low-precision-training">
<h2>Low precision training<a class="headerlink" href="#low-precision-training" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>bfloat16 (standardized in 2018)</p></li>
<li><p>FP8 (E4M3 and E5M2, standardized in 2022)</p></li>
<li><p>Fewer than 8 bits requires radical changes to training procedures</p></li>
<li><p>Still not able to get any benefits of lossy encodings (1-4 bits per sample)</p></li>
</ul>
<p><img alt="" src="_images/gradient_quant.png" /></p>
</section>
<section id="conventional-approach">
<h2>Conventional approach<a class="headerlink" href="#conventional-approach" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Decode image and convert to floating point</p></li>
<li><p>Size of input layer: <span class="math notranslate nohighlight">\(224 \times224 \times 3 \times 32\)</span> bits per image</p></li>
<li><p>For batch size of 128:</p></li>
<li><p>13 GB feature memory</p></li>
<li><p>497 GFLOPs</p></li>
</ul>
</section>
<section id="training-on-quantized-data">
<h2>Training on quantized data<a class="headerlink" href="#training-on-quantized-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Each transform coefficient only contains about 2 bits instead of 8</p></li>
<li><p>“Replace” each 2x2 blocks of 2-bit transform coefficients with a single high precision input</p>
<ul>
<li><p>Size of input layer: <span class="math notranslate nohighlight">\(112 \times 112 \times 3 \times 32\)</span> bits per image</p></li>
<li><p>3 GB Feature memory (down from 13)</p></li>
<li><p>131 GFLOPs per pass (down from 497)</p></li>
</ul>
</li>
<li><p>How do we “replace” several low-precision inputs with a single high precision input?</p></li>
<li><p>Doesn’t even work for most codecs since each subband has different quantization!</p></li>
</ul>
</section>
<section id="id2">
<h2>Training on quantized data<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>How do we “replace” several low-precision inputs with a single high precision input?</p></li>
<li><p>Naive approach: <span class="math notranslate nohighlight">\(y = (x_1) + (x2 &lt;&lt; 2) + (x2 &lt;&lt; 4) (x2 &lt;&lt; 6)\)</span></p>
<ul>
<li><p>Amounts to creating a categorical variable</p></li>
<li><p>Standard approach to training on categorical variable is to one-hot encode</p>
<ul>
<li><p>We’re back to where we started!</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="binarized-neural-networks">
<h2>Binarized neural networks<a class="headerlink" href="#binarized-neural-networks" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Proposed in 2016 to reduce memory consumption</p></li>
<li><p>Replace some floating point arithmetic with bit-wise operations</p>
<ul>
<li><p>Binary Weight Network (BWN): only the kernels are quantized</p></li>
<li><p>Binary Activation Network (BAN): only the inputs are binarized</p></li>
<li><p>Binary Neural Network (BNN): both the inputs as well as the kernels are binarized</p></li>
</ul>
</li>
<li><p>First layer of BNN is typically represented at full precision</p>
<ul>
<li><p>Gives us an opportunity to utilize low precision of lossy coded data</p></li>
</ul>
</li>
</ul>
</section>
<section id="lossy-compression-standards">
<h2>Lossy compression standards<a class="headerlink" href="#lossy-compression-standards" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Standard</p></th>
<th class="head"><p>Introduced</p></th>
<th class="head"><p>Signal</p></th>
<th class="head"><p>Transform</p></th>
<th class="head"><p>Quantization</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MPEG Layer III</p></td>
<td><p>1991</p></td>
<td><p>Audio</p></td>
<td><p>block DCT and FFT</p></td>
<td><p>Perceptual quantization vector</p></td>
</tr>
<tr class="row-odd"><td><p>JPEG</p></td>
<td><p>1992</p></td>
<td><p>Image</p></td>
<td><p>block DCT</p></td>
<td><p>Perceptual quantization matrix</p></td>
</tr>
<tr class="row-even"><td><p>JPEG 2000</p></td>
<td><p>2000</p></td>
<td><p>Image</p></td>
<td><p>Separable Wavelet</p></td>
<td><p>Uniform scalar quantization</p></td>
</tr>
<tr class="row-odd"><td><p>CELT/Opus</p></td>
<td><p>2011</p></td>
<td><p>Audio</p></td>
<td><p>block DCT</p></td>
<td><p>Pyramid vector quantization</p></td>
</tr>
<tr class="row-even"><td><p>HEVC</p></td>
<td><p>2013</p></td>
<td><p>Image</p></td>
<td><p>block DCT and DST</p></td>
<td><p>Perceptual quantization matrix</p></td>
</tr>
<tr class="row-odd"><td><p>Soundstream</p></td>
<td><p>2021</p></td>
<td><p>Audio</p></td>
<td><p>Learned</p></td>
<td><p>Residual vector quantization</p></td>
</tr>
<tr class="row-even"><td><p>Encodec</p></td>
<td><p>2022</p></td>
<td><p>Audio</p></td>
<td><p>Learned</p></td>
<td><p>Residual vector quantization</p></td>
</tr>
</tbody>
</table>
</section>
<section id="audio-classification-baseline">
<h2>Audio classification: baseline<a class="headerlink" href="#audio-classification-baseline" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Dataset: Speech commands</p>
<ul>
<li><p>one second speech segments of 8 possible words</p></li>
<li><p>‘stop,’ ‘down,’ ‘no,’ ‘right,’ ‘go,’ ‘up,’ ‘yes,’ ‘left’</p></li>
</ul>
</li>
<li><p>Baseline model:</p>
<ul>
<li><p>Input size: <span class="math notranslate nohighlight">\(128 \times 128\)</span> time-frequency distribution represented at full precision</p></li>
<li><p>119.52 MiB Feature size</p></li>
<li><p>2.26 GFLOPs per pass</p></li>
<li><p>Achieves test accuracy of about 84%</p></li>
</ul>
</li>
</ul>
</section>
<section id="audio-classification-vq-bnn">
<h2>Audio classification: VQ + BNN<a class="headerlink" href="#audio-classification-vq-bnn" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Encode 2x2 time-frequency blocks via vector quantization</p>
<ul>
<li><p>Use mini-batch k-means to learn codebook of 16 vectors (4 bits)</p></li>
<li><p>Compression ratio of 16:1 (before any entropy coding)</p></li>
</ul>
</li>
<li><p>Input size: <span class="math notranslate nohighlight">\(64 \times 64 \times 4\)</span> binary codes</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><audio controls="controls"><source src="./_static/left01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/right01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/yes01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/no01.wav" type="audio/wav"></audio></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><audio controls="controls"><source src="./_static/left01_vq.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/right01_vq.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/yes01_vq.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/no01_vq.wav" type="audio/wav"></audio></p></td>
</tr>
</tbody>
</table>
</section>
<section id="id3">
<h2>Audio classification: VQ + BNN<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Input size: <span class="math notranslate nohighlight">\(64 \times 64 \times 4\)</span> binary codes</p></li>
<li><p>3.74 MiB Feature size</p></li>
<li><p>Multiply-accumulate instead of FP</p>
<ul>
<li><p>4-way MAC unit uses about 55% of the area of a FP16 FPU</p></li>
<li><p><span class="math notranslate nohighlight">\(4-8 \times\)</span> more power efficient compared to bfloat16</p></li>
<li><p><span class="math notranslate nohighlight">\(&gt;20 \times\)</span> more power efficient compared to FP32</p></li>
</ul>
</li>
<li><p>Achieves test accuracy of about 79% (baseline:84%)</p></li>
</ul>
</section>
<section id="neural-compression-standards">
<h2>Neural compression standards<a class="headerlink" href="#neural-compression-standards" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Soundstream (Google, 2021) and Encodec (Meta, 2022)</p></li>
</ul>
<p><img alt="" src="_images/encodec_architecture.png" /></p>
</section>
<section id="audio-classification-neural-compression-bnn">
<h2>Audio classification: Neural Compression + BNN<a class="headerlink" href="#audio-classification-neural-compression-bnn" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><audio controls="controls"><source src="./_static/left01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/right01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/yes01.wav" type="audio/wav"></audio></p></th>
<th class="head"><p><audio controls="controls"><source src="./_static/no01.wav" type="audio/wav"></audio></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><audio controls="controls"><source src="./_static/left01_ecdc.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/right01_ecdc.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/yes01_ecdc.wav" type="audio/wav"></audio></p></td>
<td><p><audio controls="controls"><source src="./_static/no01_ecdc.wav" type="audio/wav"></audio></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Input size: <span class="math notranslate nohighlight">\(8 \times 75 \times 10\)</span> binary codes</p></li>
<li><p>1.67 MiB Feature size</p></li>
<li><p>111M MACs per pass</p></li>
<li><p>Test accuracy of 58%</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://papers.nips.cc/paper/2018/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html">Faster Neural Networks Straight from JPEG</a></p>
<p><a class="reference external" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.html">Deep Residual Learning in the JPEG Transform Domain</a></p>
<p><a class="reference external" href="https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf">Ultra-Low Precision 4-bit Training of Deep Neural
Networks</a></p>
<p><a class="reference external" href="https://proceedings.neurips.cc/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html">Binarized Neural Networks</a></p>
<p><a class="reference external" href="https://github.com/albanie/convnet-burden/blob/master/reports/resnet-50.md">Estimates of memory consumption and FLOP counts for various convolutional neural networks</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Information Theory and Machine Learning</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="learning_from_compressed.html">Learning from compressed Audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="discrete_representation_learning.html">Discrete Representation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="milestone.html">Compressed Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lossy Compression and Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#lossy-compression">Lossy compression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conventional-training-procedure">Conventional Training Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-on-transformed-data">Training on transformed data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Training on transformed data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#low-precision-training">Low precision training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conventional-approach">Conventional approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-on-quantized-data">Training on quantized data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Training on quantized data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#binarized-neural-networks">Binarized neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lossy-compression-standards">Lossy compression standards</a></li>
<li class="toctree-l2"><a class="reference internal" href="#audio-classification-baseline">Audio classification: baseline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#audio-classification-vq-bnn">Audio classification: VQ + BNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">Audio classification: VQ + BNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-compression-standards">Neural compression standards</a></li>
<li class="toctree-l2"><a class="reference internal" href="#audio-classification-neural-compression-bnn">Audio classification: Neural Compression + BNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="milestone.html" title="previous chapter">Compressed Learning</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/lossy_learning.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Lossy Compression and Learning\n",
    "\n",
    "[Slides](https://danjacobellis.github.io/ITML/lossy_learning.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Lossy Compression and Learning\n",
    "</h1> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> <h3>\n",
    "Dan Jacobellis\n",
    "</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1078-7e0d-44c5-8f0d-99f43c450981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conventional approach\n",
    "\n",
    "* Consider a classification problem\n",
    "  * Lossy compressed data (MP3, JPEG): 1-4 bits per sample\n",
    "  * Decoded data: 8-16 bits per sample\n",
    "  * Floating point for training: 32 bits per sample\n",
    "* Still suffers from all of the downsides of lossy compression\n",
    "* Don't get any of the benefits of smaller representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a092335-e84c-4c60-9c53-c5097c98e151",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Low precision floating point\n",
    "\n",
    "* bfloat16 (standardized in 2018)\n",
    "* FP8 (E4M3 and E5M2, standardized in 2022)\n",
    "* Fewer than 8 bits requires radical changes to training procedures\n",
    "* Still not able to get any benefits of lossy encodings (1-4 bits per sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d51b7-6044-4421-a426-8e684924118c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on compressed data\n",
    "* Consider the imagenet datasat consisting of JPEG coded images\n",
    "  * Images are 2-4 bits per pixel after quantization\n",
    "  * about 1.5 bits after entropy coding (RLE and Huffman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63fd55-ecfc-4771-ad2f-0b44d8495279",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/lossy_lossless.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d2a4f-e39f-4252-b12c-7f34b8406591",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/lossy_lossless.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50241671-5100-4080-a5f8-3d7ed048ac27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conventional approach\n",
    "* Decode image and convert to floating point\n",
    "* Size of input layer: $224 \\times224 \\times 3 \\times 32$ bits per image\n",
    "* For batch size of 128:\n",
    "* 13 GB feature memory\n",
    "* 497 GFLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822c15b-341f-4688-8845-59bcdc392842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on encoded data\n",
    "\n",
    "* Conventional approach\n",
    "  * 13 GB feature memory\n",
    "  * 497 GFLOPs per pass\n",
    "* Alternative approach\n",
    "    * Each transform coefficient only contains about 2 bits instead of 8\n",
    "    * \"Replace\" each 2x2 blocks of 2-bit transform coefficients with a single high precision input\n",
    "    * Size of input layer: $112 \\times 112 \\times 3 \\times 32$ bits per image\n",
    "    * 3 GB Feature memory\n",
    "    * 131 GFLOPs per pass\n",
    "* How do we \"replace\" several low-precision inputs with a single high precision input?\n",
    "* Doesn't even work for JPEG since each DCT coefficient has different quantization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eaa0e3-713c-400b-ad00-a9a85aa59e4d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "[resnet 50 reference](https://github.com/albanie/convnet-burden/blob/master/reports/resnet-50.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714f546-841a-428a-ba39-386ec37ae9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on encoded data\n",
    "* How do we \"replace\" several low-precision inputs with a single high precision input?\n",
    "* Naive approach: $y = (x_1) + (x2 << 2) + (x2 << 4) (x2 << 6)$\n",
    "  * Amounts to creating a categorical variable\n",
    "  * Standard approach to training on categorical variable is to one-hot encode\n",
    "    * We're back to where we started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbfdf1-b026-4a46-aa11-ffc0dce86042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Binarized neural networks\n",
    "* Proposed in 2016 to reduce memory consumption\n",
    "* Replace some floating point arithmetic with bit-wise operations\n",
    "  * Binary Weight Network (BWN): only the kernels are quantized\n",
    "  * Binary Activation Network (BAN): only the inputs are binarized\n",
    "  * Binary Neural Network (BNN): both the inputs as well as the kernels are binarized\n",
    "* First layer of BNN is typically represented at full precision\n",
    "  * Gives us an opportunity to utilize low precision of lossy coded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a30e3f-0ec2-4928-8628-89b76366955a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Types of lossy compression\n",
    "\n",
    "| Standard       | Introduced | Signal | Transform         | Quantization                   |\n",
    "|----------------|------------|--------|-------------------|--------------------------------|\n",
    "| MPEG Layer III | 1991       | Audio  | block DCT and FFT | Perceptual quantization vector |\n",
    "| JPEG           | 1992       | Image  | block DCT         | Perceptual quantization matrix |\n",
    "| JPEG 2000      | 2000       | Image  | Separable Wavelet | Uniform scalar quantization    |\n",
    "| CELT/Opus      | 2011       | Audio  | block DCT         | Pyramid vector quantization    |\n",
    "| HEVC           | 2013       | Image  | block DCT and DST | Perceptual quantization matrix |\n",
    "| Soundstream    | 2021       | Audio  | Learned           | Residual vector quantization   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4f332-8b54-4485-a3de-f612997d2791",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: baseline\n",
    "* Dataset: Speech commands\n",
    "  * one second speech segments of 8 possible words\n",
    "  * 'stop,' 'down,' 'no,' 'right,' 'go,' 'up,' 'yes,' 'left'\n",
    "* Baseline model:\n",
    "  * Input size: $128 \\times 128$ time-frequency distribution represented at full precision\n",
    "  * 119.52 MiB Feature size\n",
    "  * 2.26 GFLOPs per pass\n",
    "  * Achieves test accuracy of about 84% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a73a0-ab9b-409f-a9f9-bfa89620235c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: BNN + VQ\n",
    "* Encode 2x2 time-frequency blocks via vector quantization\n",
    "  * Use mini-batch k-means to learn codebook of 16 vectors (4 bits)\n",
    "  * Compression ratio of 16:1 (before any entropy coding)\n",
    "* Input size: $64 \\times 64 \\times 4$ binary codes\n",
    "* 3.74 MiB Feature size \n",
    "* Multiply-accumulate instead of FP\n",
    "    * $4-8 \\times$ more power efficient compared to bfloat16\n",
    "    * $>20 \\times$ more power efficient compared to FP32\n",
    "* Achieves test accuracy of about 79% (baseline:84%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3b8bb-73a7-425d-96b9-888c00360faa",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "[IBM 4-bit](https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf9578-6365-4a5c-952f-6ed0ef5c5611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

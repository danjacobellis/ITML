
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Compressed Learning &#8212; Dan Jacobellis | University of Texas at Austin</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Discrete Representation Learning" href="discrete_representation_learning.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="compressed-learning">
<h1>Compressed Learning<a class="headerlink" href="#compressed-learning" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://danjacobellis.net/ITML/_static/proposal.pdf">original proposal</a></p>
<section id="lossy-compression-standards">
<h2>Lossy compression standards<a class="headerlink" href="#lossy-compression-standards" title="Permalink to this headline">¶</a></h2>
<p>While hundreds of lossy compression techniques have been standardized, only a few have been widely deployed. We will put aside inter-frame video compression for now and focus on audio, image, and intra-frame video compression since our goal is to exploit the transform and quantization steps.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Standard</p></th>
<th class="head"><p>Introduced</p></th>
<th class="head"><p>Signal</p></th>
<th class="head"><p>Transform</p></th>
<th class="head"><p>Quantization</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MPEG Layer III</p></td>
<td><p>1991</p></td>
<td><p>Audio</p></td>
<td><p>block DCT and FFT</p></td>
<td><p>Perceptual quantization vector</p></td>
</tr>
<tr class="row-odd"><td><p>JPEG</p></td>
<td><p>1992</p></td>
<td><p>Image</p></td>
<td><p>block DCT</p></td>
<td><p>Perceptual quantization matrix</p></td>
</tr>
<tr class="row-even"><td><p>JPEG 2000</p></td>
<td><p>2000</p></td>
<td><p>Image</p></td>
<td><p>Separable Wavelet</p></td>
<td><p>Uniform scalar quantization</p></td>
</tr>
<tr class="row-odd"><td><p>CELT/Opus</p></td>
<td><p>2011</p></td>
<td><p>Audio</p></td>
<td><p>block DCT</p></td>
<td><p>Pyramid vector quantization</p></td>
</tr>
<tr class="row-even"><td><p>HEVC</p></td>
<td><p>2013</p></td>
<td><p>Image</p></td>
<td><p>block DCT and DST</p></td>
<td><p>Perceptual quantization matrix</p></td>
</tr>
<tr class="row-odd"><td><p>Soundstream</p></td>
<td><p>2021</p></td>
<td><p>Audio</p></td>
<td><p>Learned</p></td>
<td><p>Residual vector quantization</p></td>
</tr>
</tbody>
</table>
<section id="transforms">
<h3>Transforms<a class="headerlink" href="#transforms" title="Permalink to this headline">¶</a></h3>
<p>The block DCT and its variants dominate among commonly used codecs. However, emerging standards use learned encoders. <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9625818?casa_token=1Qcv6BHkWiIAAAAA:f7tDFZAEL_cVlsrj5TqVFV1pt-dSb_J6-ju0XcBlwGcccnUTVz7TJCfqqVkEKlzKZVTPYDFo">Soundstream [1]</a> (by Google) and <a class="reference external" href="https://arxiv.org/abs/2210.13438">Encodec [2]</a> (by Meta), have recently become standardized. Both are based on the vector quantized variational autoencoder (VQ-VAE). Due to the computational cost of the encoder and decoder, it may seem implausible that these learned codecs are widely adopted. But considering that <a class="reference external" href="https://liuliu.me/eyes/stretch-iphone-to-its-limit-a-2gib-model-that-can-draw-everything-in-your-pocket">large models like Stable Diffusion can run on an iPhone [3]</a>, the computational cost may eventually be reasonable.</p>
</section>
<section id="quantization">
<h3>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">¶</a></h3>
<p>The dominant approach to quantization is to define a perceptual quantization matrix (or vector) <span class="math notranslate nohighlight">\(Q\)</span> and perform the simple quantization <span class="math notranslate nohighlight">\(B = round(\frac{G}{Q})\)</span>, where <span class="math notranslate nohighlight">\(G\)</span> is the transformed signal and <span class="math notranslate nohighlight">\(B\)</span> is the quantized representation. In CELT (now integrated into the OPUS standard) transform coefficients are normalized to one. This unit vector is then coded separately from the magnitude, which allows the use of pyramid vector quantization which is much more efficient but only applicable to unit vectors. In the learned codecs (Soundstream and Encodec) several low-complexity vector quantizers are used, each predicting the residual of the previous.</p>
</section>
</section>
<section id="integrating-compression-and-learning">
<h2>Integrating compression and learning<a class="headerlink" href="#integrating-compression-and-learning" title="Permalink to this headline">¶</a></h2>
<p>Consider JPEG, where DCT coefficients are generally quantized to between 2-5 bits per pixel but the standard training approach represents all inputs using 32-bit floating point values.</p>
<p>It would be nice if we could simply represent 2x2 or perhaps even 4x4 blocks using a single 32-bit floating point number, thus reducing the size of the input layer by an order of magnitude, and simplifying training. However, this would amount to encoding blocks as a categorical variable. Since the standard approach to learning a categorical variable is to one-hot encode, we are back to where we started!</p>
<p>Recently, <a class="reference external" href="https://arxiv.org/abs/2209.05433">approaches to training with lower precision representations [4]</a> have been explored, but it is unlikely that anything fewer than 8 bits can be used without drastically changing the training procedure.</p>
<section id="post-training-quantization-and-quantization-aware-training">
<h3>Post training quantization and quantization aware training<a class="headerlink" href="#post-training-quantization-and-quantization-aware-training" title="Permalink to this headline">¶</a></h3>
<p>The demand to perform inference of deep learning models on mobile and embedded devices has resulted in many techniques for model quantization. With a few clever techniques, it is possible to quantize the weights of a traditionally trained network to low precision (e.g. 8 bits) and retain good performance. To quantize weights even further to 4 or fewer bits, <a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html">techniques for quantization aware training have been proposed[5].</a> Unfortunately, the term “quantization-aware” only refers the fact that the network will eventually be quantized, not the fact that the training inputs are quantized. Thus, these techniques are not immediately applicable to reduce the cost of training by exploiting the quantization of encoded inputs.</p>
</section>
<section id="binary-neural-networks">
<h3>Binary neural networks<a class="headerlink" href="#binary-neural-networks" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://proceedings.neurips.cc/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html">Binary neural networks[6]</a>  are a more drastic adaptation to aimed to reduce model size. One limitation of BNNs is that the <a class="reference external" href="https://docs.larq.dev/larq/guides/bnn-architecture/#first-last-layer">first input layer must typically be represented at full precision [7][8]</a>, while the remaining signal path can be represented using binary inputs, weights, and activations.</p>
<p>With the traditional training setup, it did not provide an advantage to represent groups of low-precision inputs using a categorical variable since it would be expand it anyway (e.g. using one-hot encoding). However, with a binary neural network, this expansion is less of a cost because it allows us to reduce the input layer from full precision.</p>
</section>
</section>
<section id="experiment-4x4-image-blocks-encoded-to-4-bits-as-input-to-bnn">
<h2>Experiment: 4x4 image blocks encoded to 4 bits as input to BNN<a class="headerlink" href="#experiment-4x4-image-blocks-encoded-to-4-bits-as-input-to-bnn" title="Permalink to this headline">¶</a></h2>
<p>In this experiment, we encode 4x4 blocks of MNIST digit images to 4-bit codes, a reduction in representation size of 128x. We train two BNNs:</p>
<ul class="simple">
<li><p>The first network, which will act as the control in the experiment, is trained on the original MNIST images and the input layer is represented using full precision.</p></li>
<li><p>The second network is trained on lossy 4-bit codes and the input layer is binary.</p></li>
</ul>
<p>We will use a VQ-VAE trained on MNIST as our “transform + quantization” lossy compression algorithm. We use four latent dimensions and allow 16 possible codebook vectors (i.e. 4 bit codes) in our VQ-VAE to represent each 4x4 image block.</p>
<section id="results">
<h3>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h3>
<p>The baseline network reaches about 98% accuracy after 10 epochs, requires 2,599,552 1-bit MACs, and 194,688 32-bit MACs.</p>
<p>The network with binary inputs only reaches 85% accuracy after 10 epochs, but only requires 4,736 1-bit MACs and 12,544 32-bit MACs.</p>
<p>The number of model parameters is reduced from 93,000 to 17,000, and the overall model size is reduced from 365 KiB to 68 KiB</p>
</section>
</section>
<section id="next-steps">
<h2>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">¶</a></h2>
<p>The general approach of using BNNs on transformed and quantized representations is promising. Subsequent work will focus on:</p>
<ul class="simple">
<li><p>Adapting the technique to audio signals. MNIST was used initially for convenience, since even the simplest audio datasets have large input dimensions.</p></li>
<li><p>Testing the performance on other, more widely used codecs. A VQ-VAE was used because it provides an extremely high compression ratio where the results would be clearly visible.</p></li>
<li><p>Testing different ways to modify the network architecture when the input size is reduced.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9625818?casa_token=1Qcv6BHkWiIAAAAA:f7tDFZAEL_cVlsrj5TqVFV1pt-dSb_J6-ju0XcBlwGcccnUTVz7TJCfqqVkEKlzKZVTPYDFo">SoundStream: An End-to-End Neural Audio Codec</a></p>
<p>[2] <a class="reference external" href="https://arxiv.org/abs/2210.13438">High Fidelity Neural Audio Compression</a></p>
<p>[3] <a class="reference external" href="https://liuliu.me/eyes/stretch-iphone-to-its-limit-a-2gib-model-that-can-draw-everything-in-your-pocket/">Stretch iPhone to its limit: 2GiB Stable Diffusion model runs locally on device</a></p>
<p>[4] <a class="reference external" href="https://arxiv.org/abs/2209.05433">FP8 Formats for Deep Learning</a></p>
<p>[5] <a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></p>
<p>[6] <a class="reference external" href="https://proceedings.neurips.cc/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html">Binarized Neural Networks</a></p>
<p>[7] <a class="reference external" href="https://joss.theoj.org/papers/10.21105/joss.01746">Larq: An Open-Source Library for Training Binarized Neural Networks</a></p>
<p>[8] <a class="reference external" href="https://docs.larq.dev/larq/guides/bnn-architecture/#first-last-layer">Larq documentation. “Building Binary Neural Networks: First and Last Layers”</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Information Theory and Machine Learning</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="learning_from_compressed.html">Learning from compressed Audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="discrete_representation_learning.html">Discrete Representation Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Compressed Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#lossy-compression-standards">Lossy compression standards</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integrating-compression-and-learning">Integrating compression and learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#experiment-4x4-image-blocks-encoded-to-4-bits-as-input-to-bnn">Experiment: 4x4 image blocks encoded to 4 bits as input to BNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#next-steps">Next steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="discrete_representation_learning.html" title="previous chapter">Discrete Representation Learning</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/milestone.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
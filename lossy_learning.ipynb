{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Learning from Lossy Encoded Data\n",
    "[Slides](https://danjacobellis.github.io/ITML/lossy_learning.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Learning from Lossy Encoded Data\n",
    "</h1> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> <h2>\n",
    "Dan Jacobellis\n",
    "</h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1078-7e0d-44c5-8f0d-99f43c450981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression\n",
    "\n",
    "* Most data are stored using lossy formats (MP3, JPEG)\n",
    "* 1-4 bit subband quantization is typical\n",
    "* ~1.5 bits per sample/pixel after entropy coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b6296-15c4-49c9-a70b-01362df4bdfd",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/lossy_lossless.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415e03e-48e4-41da-b49d-89bc78f7f7f6",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/lossy_lossless.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012104d-ab48-462e-a8ca-2bc3788cc92a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conventional Training Procedure\n",
    "\n",
    "* Still suffers from all of the downsides of lossy compression\n",
    "* Don't get any of the benefits of smaller representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42577d58-eb38-4c27-9c6c-61568a1ad349",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/conventional.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f030fd9-c281-48e6-96ef-61cce8442f94",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/conventional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856118da-3465-40a3-88b7-625fd50168cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on transformed data\n",
    "\n",
    "* Reduce size and number of initial convolutional layers by training on DCT coefficients\n",
    "* Input size remains the same (Quantization of transform coefficients ignored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbc5e8-f1c6-4734-8b1d-9452529dde11",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/dct_vs_resnet.png\" width=800 height=800 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643df2f-1b30-4caa-b2a5-ded49d17641f",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/dct_vs_resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd84e14-d5d3-44f8-82ca-ddadbcc77fef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad962697-19d5-4367-9747-9fadd9b810b0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/FNNSFJ.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb969d9e-28df-462e-b7f5-3557a6d0d601",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/FNNSFJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0745d-5261-4b1b-b288-ec7e8cec0b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Low precision training\n",
    "\n",
    "* bfloat16 (standardized in 2018)\n",
    "* FP8 (E4M3 and E5M2, standardized in 2022)\n",
    "* Fewer than 8 bits requires radical changes to training procedures\n",
    "* Still not able to get any benefits of lossy encodings (1-4 bits per sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59b249-ad3d-4fb8-a8e6-be7b97fff0fd",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/gradient_quant.png\" width=800 height=800 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f14e8-5503-482a-9bdf-d90ed6f1d56e",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/gradient_quant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358e66b-d377-4456-aeec-821b0b28ef49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Cost of conventional approach\n",
    "\n",
    "* Example: Imagenet\n",
    "    * Decode image and convert to floating point\n",
    "    * Size of input layer: $224 \\times224 \\times 3 \\times 32$ bits per image\n",
    "    * For batch size of 128:\n",
    "        * 13 GB feature memory\n",
    "        * 497 GFLOPs per pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822c15b-341f-4688-8845-59bcdc392842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on quantized data\n",
    "\n",
    "* Each transform coefficient only contains about 2 bits instead of 8\n",
    "* \"Replace\" each 2x2 block of 2-bit transform coefficients with a single high precision input\n",
    "  * Size of input layer: $112 \\times 112 \\times 3 \\times 32$ bits per image\n",
    "  * 3 GB Feature memory (down from 13)\n",
    "  * 131 GFLOPs per pass (down from 497)\n",
    "* How do we \"replace\" several low-precision inputs with a single high precision input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714f546-841a-428a-ba39-386ec37ae9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training on quantized data\n",
    "* How do we \"replace\" several low-precision inputs with a single high precision input?\n",
    "* Naive approach: $y = (x_1) + (x2 << 2) + (x2 << 4) (x2 << 6)$\n",
    "  * Won't work for most codecs since each subband has different quantization\n",
    "  * Amounts to creating a categorical variable\n",
    "  * Standard approach to training on categorical variable is to one-hot encode\n",
    "    * We're back to where we started!    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbfdf1-b026-4a46-aa11-ffc0dce86042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Binarized neural networks\n",
    "* Proposed in 2016 as a way to reduce memory consumption\n",
    "* Replace some floating point arithmetic with bit-wise operations\n",
    "  * Binary Weight Network (BWN): only the kernels are quantized\n",
    "  * Binary Activation Network (BAN): only the inputs are binarized\n",
    "  * Binary Neural Network (BNN): both the inputs as well as the kernels are binarized\n",
    "* First layer of BNN is typically represented at full precision\n",
    "  * Gives us an opportunity to utilize low precision of lossy coded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a30e3f-0ec2-4928-8628-89b76366955a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression standards\n",
    "\n",
    "| Standard       | Introduced | Signal | Transform         | Quantization                   |\n",
    "|----------------|------------|--------|-------------------|--------------------------------|\n",
    "| MPEG Layer III | 1991       | Audio  | block DCT and FFT | Perceptual quantization vector |\n",
    "| JPEG           | 1992       | Image  | block DCT         | Perceptual quantization matrix |\n",
    "| JPEG 2000      | 2000       | Image  | Separable Wavelet | Uniform scalar quantization    |\n",
    "| CELT/Opus      | 2011       | Audio  | block DCT         | Pyramid vector quantization    |\n",
    "| HEVC           | 2013       | Image  | block DCT and DST | Perceptual quantization matrix |\n",
    "| Soundstream    | 2021       | Audio  | Learned           | Residual vector quantization   |\n",
    "| Encodec        | 2022       | Audio  | Learned           | Residual vector quantization   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4f332-8b54-4485-a3de-f612997d2791",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: baseline\n",
    "* Dataset: Speech commands\n",
    "  * One second speech segments of 8 possible words\n",
    "  * 'stop,' 'down,' 'no,' 'right,' 'go,' 'up,' 'yes,' 'left'\n",
    "* Baseline model:\n",
    "  * Input size: $128 \\times 128$ time-frequency distribution represented at full precision\n",
    "  * 119.52 MiB Feature size\n",
    "  * 2.26 GFLOPs per pass\n",
    "  * Achieves test accuracy of about 84% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a73a0-ab9b-409f-a9f9-bfa89620235c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: VQ + BNN\n",
    "* Encode 2x2 time-frequency blocks via vector quantization\n",
    "  * Use mini-batch k-means to learn codebook of 16 vectors (4 bits)\n",
    "  * Compression ratio of 16:1 (before any entropy coding)\n",
    "* Input size: $64 \\times 64 \\times 4$ binary codes\n",
    "\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/right01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/yes01.wav\" type=\"audio/wav\"></audio>    | <audio controls=\"controls\"><source src=\"./_static/no01.wav\" type=\"audio/wav\"></audio>    |\n",
    "|--------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/right01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/yes01_vq.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/no01_vq.wav\" type=\"audio/wav\"></audio> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa709437-d2f7-49f3-9d8b-854c38891da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: VQ + BNN\n",
    "* Input size: $64 \\times 64 \\times 4$ binary codes\n",
    "* 3.74 MiB feature size \n",
    "* Multiply-accumulate instead of FP\n",
    "  * 4-way MAC unit uses about 55% of the area of a FP16 FPU\n",
    "  * $4-8 \\times$ more power efficient compared to bfloat16\n",
    "  * $>20 \\times$ more power efficient compared to FP32\n",
    "* Achieves test accuracy of about 79% (down from baseline of 84%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419c6ad-7b36-4226-a9e4-1095874b977f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Neural compression standards\n",
    "\n",
    "* Soundstream (Google, 2021) and Encodec (Meta, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aac0c2-aff8-4c97-bb18-cd8b3532f378",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/encodec_architecture.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a153e-0cc8-4ceb-a02a-6f15b3535cce",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/encodec_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d309d4f-8ef6-4b28-a573-00e48d8c0ef4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Audio classification: Neural Compression + BNN\n",
    "\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01.wav\" type=\"audio/wav\"></audio>      | <audio controls=\"controls\"><source src=\"./_static/right01.wav\" type=\"audio/wav\"></audio>      | <audio controls=\"controls\"><source src=\"./_static/yes01.wav\" type=\"audio/wav\"></audio>      | <audio controls=\"controls\"><source src=\"./_static/no01.wav\" type=\"audio/wav\"></audio>      |\n",
    "|----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| <audio controls=\"controls\"><source src=\"./_static/left01_ecdc.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/right01_ecdc.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/yes01_ecdc.wav\" type=\"audio/wav\"></audio> | <audio controls=\"controls\"><source src=\"./_static/no01_ecdc.wav\" type=\"audio/wav\"></audio> |\n",
    "\n",
    "* Input size: $8 \\times 75 \\times 10$ binary codes\n",
    "* 1.67 MiB Feature size \n",
    "* 111M MACs per pass\n",
    "* Test accuracy of 58%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf9578-6365-4a5c-952f-6ed0ef5c5611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "[Faster Neural Networks Straight from JPEG](https://papers.nips.cc/paper/2018/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html)\n",
    "\n",
    "[Deep Residual Learning in the JPEG Transform Domain](https://openaccess.thecvf.com/content_ICCV_2019/html/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.html)\n",
    "\n",
    "[Ultra-Low Precision 4-bit Training of Deep Neural\n",
    "Networks](https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf)\n",
    "\n",
    "[Binarized Neural Networks](https://proceedings.neurips.cc/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html)\n",
    "\n",
    "[Estimates of memory consumption and FLOP counts for various convolutional neural networks](https://github.com/albanie/convnet-burden/blob/master/reports/resnet-50.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

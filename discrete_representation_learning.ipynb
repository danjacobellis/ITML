{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Discrete Representation Learning\n",
    "\n",
    "[Paper](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)\n",
    "\n",
    "[Slides](https://danjacobellis.github.io/ITML/discrete_representation_learning.slides.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Discrete Representation Learning\n",
    "</h1> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> <h3>\n",
    "Dan Jacobellis\n",
    "</h3> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\\[1\\] [Neural discrete representation learning.](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html) Van Den Oord et al. NIPS 2017.\n",
    "\n",
    "\\[2\\] [Generating Diverse High-Fidelity Images\n",
    "with VQ-VAE-2](https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf) Razavi et al. NIPS 2019.\n",
    "\n",
    "\\[3\\] [Jukebox: A Generative Model for Music.](https://openai.com/blog/jukebox/) Dhariwal et al. OpenAI 2020.\n",
    "\n",
    "\\[4\\] [Zero-Shot Text-to-Image Generation.](http://proceedings.mlr.press/v139/ramesh21a.html?ref=https://githubhelp.com) Ramesh et al. PMLR 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c63c1c-330b-47d1-a4f8-d29f3c6941c8",
   "metadata": {},
   "source": [
    "## Discrete representations\n",
    "\n",
    "* Abstract away noise and detail\n",
    "* Make the representation discrete to match real world\n",
    "  * Language is naturally discrete\n",
    "  * Speech and music can be represented by sequence of symbols\n",
    "  * Images can be described by composing objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563e94b-b7ea-4dd1-9541-9b9ec85ad061",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Quantization\n",
    "\n",
    "* Suppose we have a randomly sampled vector $x$ from some distribution.\n",
    "  * Example: $x$ is a digital audio recording, image, or video.\n",
    "* Goal: create a codebook of $k$ vectors $c_k$ so that $x$ is close to at least one of the vectors in the codebook.\n",
    "\n",
    "$$k^* = \\text{arg} \\min_{k}{\\lVert x-c_k\\rVert ^2}$$\n",
    "\n",
    "$$x \\approx c_{k^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14b23a-abc0-4139-a777-9d7d86db5396",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/vq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dbb52-3a89-43aa-9052-198d65ae1a33",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/vq.png\" width=300 height=300 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c900c-b80f-4c7a-9fcf-d19c74b18b19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb587588-c9e7-439b-9fae-7ca96469f2b8",
   "metadata": {},
   "source": [
    "* Simple and effective form of lossy compression\n",
    "* Still used today for high quality audio compression\n",
    "* Finding large codebooks is difficult \n",
    "\n",
    "$$\\{c_k^*\\} := \\arg\\min_{\\{c_k\\}} \\sum_{h=1}^N \\min_k\n",
    "\\|x_h-c_k\\|^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d13f7-952e-490a-8cc5-282cd4e8a4df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "* Encoder parameterises a posterior $q(z|x)$\n",
    "* Decoder's distribution over input data is $p(x|z)$\n",
    "* Prior distribution $p(z)$ is Gaussain with diagonal covariance\n",
    "  * Allows us to take advantage of reparameterization trick\n",
    "  \n",
    "![](img/VAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a8ac0-ef64-4b94-b81e-71f06e5cd934",
   "metadata": {},
   "source": [
    "## Autoregressive models\n",
    "\n",
    "![](img/autoregressive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bddef-fea9-442b-b501-eb24ae3db40f",
   "metadata": {},
   "source": [
    "## Autoregressive models\n",
    "\n",
    "* Extremely powerful autoregressive models introduced prior to this work\n",
    "\n",
    "![](img/autoregressive_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced764d9-814b-4d66-a388-55eb8d4e1071",
   "metadata": {},
   "source": [
    "## Posterior Collapse\n",
    "\n",
    "* Optimizing ELBO does not guarantee mutual information between input $x$ and latent $z$\n",
    "* A strong autoregressive decoder may ignore latents $z$\n",
    "\n",
    "$$q(z|x)\\approx q(z)=\\mathcal N(a,b)$$\n",
    "\n",
    "* This was a signifant issue in previous attempts to combine VAE with autoregressive models\n",
    "* Many different solutions have been proposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399eda-6cdd-4026-be6a-33669e16d5da",
   "metadata": {},
   "source": [
    "## Vector-quantized autoencoder\n",
    "\n",
    "* Discrete uniform prior instead of Gaussian\n",
    "* Latent embedding space consists of codebook: $e\\in R^{K\\times D}$\n",
    "* Posterior $q(z|x)$ is a set of codes\n",
    "  \n",
    "![](img/vqvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be1271-a41b-4a3c-8934-668413f89c00",
   "metadata": {},
   "source": [
    "## Discrete latent variables\n",
    "\n",
    "* Output of encoder is $z_e(x)$ is quantized to nearest code\n",
    "* Nearest neighbor look-up in the embedding space $e$\n",
    "\n",
    "$$k=\\text{arg}\\min_{j}{\\lVert z_e(x) - e_j \\rVert_2}$$\n",
    "\n",
    "$$q(z|x)=\\begin{cases}1 & \\text{for } z=k \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "  \n",
    "![](img/encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc93de-dbe9-41ec-98c2-a8c2baa1a614",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "* Input to decoder is the embedding vector $z_q(x)=e_k$\n",
    "\n",
    "* Less sensitive to posterior collapse so powerful autoregressive decoder can be used.\n",
    "\n",
    "![](img/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e89ac7-7e8a-4af1-9b26-3282f71cc249",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "* The quantization step is not differentiable\n",
    "  * Just copy the gradients from the decoder input to encoder output instead\n",
    "  * Since the output of the encoder and the input to the decoder share the same $D$ dimensional space, there is still useful information in the gradient.\n",
    "  * We will need to additional term in loss function to learn the codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1d122-bc48-4612-97a4-6befe22114f2",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "$$L = \\underbrace{\\log (p(x|z_q(x))}_{\\text{reconstruction loss}}+\\underbrace{\\lVert \\text{sg}[z_e(x)] - e \\rVert_2^2}_{\\text{vector quantization}}+\\underbrace{\\beta \\lVert z_e(x) - \\text{sg}[e] \\rVert_2^2}_{\\text{commitment loss}}$$\n",
    "\n",
    "<center> (stopgradient operator $\\text{sg}[\\cdot]$ forces the operand to be a non-updated constant) </center>\n",
    "\n",
    "* Codebook loss ensures selected codes are close to the output of the encoder\n",
    "* Commitment loss with hyperparameter $\\beta$\n",
    "  * Encourages the output of the encoder to stay close to the chosen codebook vector\n",
    "  * Helps avoid fluctuating between code vectors when the encoder trains faster than the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e70aa-63e8-499d-a567-dfbbf1278ddc",
   "metadata": {},
   "source": [
    "## Hierarchical VQ-VAE\n",
    "![](img/audio_hvqvae.png)\n",
    "![](img/image_hvqvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36866c5-c8e4-4668-9fa1-5b0d88217632",
   "metadata": {},
   "source": [
    "## Random restarts\n",
    "\n",
    "* Although discretizing helps with the posterior collapse there it is not perfect.\n",
    "\n",
    "* One technique that has been used is random restarts\n",
    "\n",
    "* If a vector in the codebook is not being used, randomly reset it to one of the encoder outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb726b10-9189-4074-b104-426aa996672a",
   "metadata": {},
   "source": [
    "## Time-frequency loss\n",
    "\n",
    "* When the loss function compares the input an output samples directly, there is a tendency to favor low frequencies\n",
    "\n",
    "* A solution is to compute the loss on a time-frequency representation of the signal\n",
    "\n",
    "* Even better if the loss is calculated using TF representations with varying time and frequency resolutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80393244-ca7f-4071-82a9-9554c4987926",
   "metadata": {},
   "source": [
    "# Compressed Learning\n",
    "\n",
    "[original proposal](https://danjacobellis.net/ITML/_static/proposal.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908ab55-ed7f-4077-aa95-bb8d9e896423",
   "metadata": {},
   "source": [
    "## Lossy compression standards\n",
    "\n",
    "While hundereds of lossy compression techniques have been standardized, only a few have been widely deployed. We will put aside inter-frame video compression for now and focus on audio, image, and intra-frame video compression since our goal is to exploit the transform and quantization steps.\n",
    "\n",
    "| Standard       | Introduced | Signal | Transform         | Quantization                   |\n",
    "|----------------|------------|--------|-------------------|--------------------------------|\n",
    "| MPEG Layer III | 1991       | Audio  | block DCT and FFT | Perceptual quantization vector |\n",
    "| JPEG           | 1992       | Image  | block DCT         | Perceptual quantization matrix |\n",
    "| JPEG 2000      | 2000       | Image  | Separable Wavelet | Scalar quantization            |\n",
    "| CELT/Opus      | 2011       | Audio  | block DCT         | Pyramid vector quantization    |\n",
    "| HEVC           | 2013       | Image  | block DCT and DST | Perceptual quantization matrix |\n",
    "| Soundstream    | 2021       | Audio  | Learned           | Residual vector quantization   |\n",
    "\n",
    "</center> Table 1. Summary of transform and quantization techniques used in some lossy compression standards. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d226ce1-0c4d-4d03-aa24-bdc3123ea42c",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "The block DCT and its variants dominate among commonly used codecs. Some notable exceptions are:\n",
    "\n",
    "* JPEG 2000 (which is the standard for digital cinema where inter-frame compression is forbidden). The separable wavelet transform used in JPEG 2000 still suffers many of the sample limitations of DCT-based transforms. Although complex wavelet transforms allow sparse representations of singularities or oriented components, none have become popular for lossy compression.\n",
    "* Low rate speech codecs focus on modeling properties of human voice (pitch, formants, etc) but are not suitable for other types of signals like music.\n",
    "* Emerging standards which use learned transforms. Soundstream (Google) and Encodec (META), based on the vector quantized VAE, have recently become standardized. It's possible that learned image and video codecs will soon become standardized as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7694f-8fd2-48bb-a6c4-8c4cacdc9b72",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Tthe dominant approach to quantization is to define a perceptual quantization matrix (or vector) $Q$ and perform the simple quantization $B = round(\\frac{G}{Q})$, where $G$ is the transformed signal and $B$ is the quantizated representation.\n",
    "\n",
    "However, variants of vector quantization are now being deployed in standards:\n",
    "\n",
    "* CELT (now integrated into the OPUS standard) normalizes transform coefficients to one, then separately codes the unit vector from the magnitude. This allows the use of pyramid vector quantization which is much more efficient but only applicable to unit vectors.\n",
    "* Soundstream uses several low-complexity vector quantizers, each predicting the residual of the previous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2f8f3-67f2-4c0f-a11a-b7f3dd8ad3a4",
   "metadata": {},
   "source": [
    "## Why integrate compression and learning?\n",
    "\n",
    "Consider the task of image classification on ImageNet or a similar dataset.\n",
    "\n",
    "If 1000 classes are represented in the dataset and each class is equally likely, then the class information amounts to about $\\log_2(1000)\\approx 10$ bits per image.\n",
    "\n",
    "Now consider the original AlexNet architecture. Strided convolutional layers and pooling reduce the dimensionalty from $224\\times224\\times3$ at the input down to $3\\times3\\times256$.\n",
    "\n",
    "We know that since this model can acheives about 65% accuracy, most of the class information (about 9 out of the 10 bits) is preserved under this transformation. \n",
    "\n",
    "Since all values are represented as 32-bit floating point during training, we have increased the ratio of class information relative to the number of bits used to represent the image by a factor of 50.\n",
    "\n",
    "$$\\frac{\\text{Bits encoding class at input}}{\\text{Bits used to represent image at the input}} = \\frac{10}{224\\times224\\times3\\times32} \\approx 2.1\\times 10^{-6}$$\n",
    "\n",
    "$$\\frac{\\text{Bits encoding class at output}}{\\text{Bits used to represent image at the output}} = \\frac{9}{3\\times3\\times256\\times32} \\approx 1.2\\times 10^{-4}$$\n",
    "\n",
    "This increase in density greatly reduces the cost of learning the fully connected layers which predict the image class.\n",
    "\n",
    "However, data from ImageNet are not losslessly coded HDR images with 96 bits per pixel! Instead, they were JPEG encoded with around 3 bits per pixel.\n",
    "\n",
    "$$\\frac{\\text{Bits encoding class at input}}{\\text{Bits required to encode JPEG image}} = \\frac{10}{224\\times224\\times3} \\approx 6.6\\times 10^{-5}$$\n",
    "\n",
    "By preserving this density of information we may be able to reduce the resources required by the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6ff1e-b30b-4b63-a885-5366380827d7",
   "metadata": {},
   "source": [
    "\n",
    "## Standardized ML compression techniques\n",
    "\n",
    "## Natural signal representation modeling\n",
    "\n",
    "## Approaches to integrating compression and learning\n",
    "\n",
    "## Comparison of learned filters with conventional transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed05ae-c7dd-40aa-a9ba-74498ae9575d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Dictionaries for Sparse Representation Modeling (2010)](https://ieeexplore.ieee.org/abstract/document/5452966)\n",
    "\n",
    "[End-to-end Optimized Image Compression (2017)](https://www.cns.nyu.edu/~lcv/iclr2017/)\n",
    "\n",
    "[SoundStream: An End-to-End Neural Audio Codec (2022)](https://ieeexplore.ieee.org/abstract/document/9625818)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
